from pathlib import Path
import os
import requests
from typing import List, Dict, Any, Optional, Iterator
from PIL import Image
import re
import torch

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
# from llama_index.core.postprocessor import LLMRerank
from llama_index.core.tools import FunctionTool, QueryEngineTool, ToolMetadata
from llama_index.core.agent import ReActAgent

from openai import OpenAI

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


import streamlit as st


class PromptEngineerAgent:
    """ä¸“é—¨ç”¨äºä¼˜åŒ–æç¤ºè¯çš„ä»£ç†"""
    def __init__(self, llm):
        self.llm = llm
        
    def optimize_image_prompt(self, user_input: str) -> str:
        """
        å°†ç”¨æˆ·çš„å›¾åƒéœ€æ±‚è½¬æ¢ä¸ºä¼˜åŒ–çš„stable-diffusionæç¤ºè¯
        """
        prompt_template = f"""
        è¯·å°†ä»¥ä¸‹ç”¨æˆ·çš„å›¾åƒéœ€æ±‚è½¬æ¢ä¸ºstable-diffusionæ‰€éœ€çš„æ–‡ç”Ÿå›¾æç¤ºè¯ã€‚
        
        ç”¨æˆ·éœ€æ±‚: {user_input}
        
        è¯·ç”Ÿæˆä¸€ä¸ªä¼˜åŒ–çš„è‹±æ–‡æç¤ºè¯ï¼Œæ ¼å¼è¦æ±‚ï¼š
        1. ä½¿ç”¨è¯¦ç»†çš„æè¿°æ€§è¯­è¨€
        2. åŒ…å«å…·ä½“çš„è‰ºæœ¯é£æ ¼
        3. è¯´æ˜æ„å›¾å’Œè§†è§’
        4. æè¿°å…‰å½±å’Œæ°›å›´
        5. æ·»åŠ ç›¸å…³çš„è‰ºæœ¯å®¶å‚è€ƒæˆ–é£æ ¼ç±»å‹
        
        æç¤ºè¯:
        """
        
        response = self.llm.complete(prompt_template)
        return str(response)
    
    def optimize_voice_prompt(self, user_input: str) -> Dict[str, str]:
        """
        ä¼˜åŒ–è¯­éŸ³åˆæˆçš„å‚æ•°
        """
        prompt_template = f"""
        è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œå¹¶æä¾›ä¼˜åŒ–çš„è¯­éŸ³åˆæˆå‚æ•°ã€‚
        
        æ–‡æœ¬: {user_input}
        
        è¯·è€ƒè™‘ï¼š
        1. æœ€é€‚åˆçš„è¯­è¨€
        2. è¯´è¯çš„è¯­é€Ÿ
        3. è¯­æ°”ç‰¹ç‚¹
        4. æƒ…æ„Ÿè‰²å½©
        
        ä»¥JSONæ ¼å¼è¿”å›å‚æ•°ï¼š
        """
        
        response = self.llm.complete(prompt_template)
        try:
            params = eval(str(response))
            return params
        except:
            return {"lang": "zh", "speed": 1.0}


class MultiModalAssistant:
    def __init__(self, data_source_dir, llm, api_key):
        """
        åˆå§‹åŒ–åŠ©æ‰‹ï¼Œè®¾ç½®å¿…è¦çš„APIå¯†é’¥å’ŒåŠ è½½æ–‡æ¡£
        """
        
        # åˆå§‹åŒ–LLM
        self.llm = llm
        self.__api_key = api_key
        # åˆå§‹åŒ–Prompt Engineer Agent
        self.prompt_engineer = PromptEngineerAgent(self.llm)
        
        # åŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºç´¢å¼•
        documents = SimpleDirectoryReader(data_source_dir, recursive=False, required_exts=[".txt"]).load_data()
        self.index = VectorStoreIndex.from_documents(
            documents
        )
        
        # åˆ›å»ºrag ç”¨äºå›ç­”çŸ¥è¯†é—®é¢˜
        self.query_engine = self.index.as_query_engine(similarity_top_k=3)

        # åˆ›å»ºrag+rerankerç”¨äºå›ç­”çŸ¥è¯†é—®é¢˜
        # self.query_engine = self.index.as_query_engine(similarity_top_k=3,
        #                                         node_postprocessors=[
        #                                         LLMRerank(
        #                                             choice_batch_size=5,
        #                                             top_n=2,
        #                                         )],
        #                                         response_mode="tree_summarize",)        
        # è®¾ç½®å·¥å…·
        tools = [
            FunctionTool.from_defaults(
                fn=self.rag_query,
                name="rag_tool",
                description="æ— æ³•ç›´æ¥å›ç­”æ—¶ï¼ŒæŸ¥è¯¢å’Œã€Šé»‘ç¥è¯ï¼šæ‚Ÿç©ºã€‹æœ‰å…³çŸ¥è¯†çš„å·¥å…·"
            ),
            FunctionTool.from_defaults(
                fn=self.text_to_speech,
                name="tts_tool",
                description="å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³çš„å·¥å…·"
            ),
            FunctionTool.from_defaults(
                fn=self.generate_image,
                name="image_tool",
                description="ç”Ÿæˆå›¾åƒçš„å·¥å…·"
            )
        ]
        
        # åˆå§‹åŒ–Agent
        self.agent = ReActAgent.from_tools(
            tools,
            llm=self.llm,
            verbose=True,
            max_function_calls=5,
        )

        ## ç”»å›¾çš„url
        self.image_url = None
        self.audio_save_file = "audio.mp3"
        self.audio_text = None
        
    def rag_query(self, query: str) -> str:
        """
        ä½¿ç”¨RAGç³»ç»ŸæŸ¥è¯¢çŸ¥è¯†åº“
        """
        response = self.query_engine.query(query)
        return str(response)
    
    def text_to_speech(self, text: str) -> str:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³
        """
        if not self.audio_text is None:
            print(f"æ–‡æœ¬å·²è½¬ä¸ºè¯­éŸ³:  {self.audio_text}")
            return 
        try:
            client = OpenAI( api_key = self.__api_key, base_url="https://api.siliconflow.cn/v1")

            with client.audio.speech.with_streaming_response.create(
            model="fishaudio/fish-speech-1.5", # ç›®å‰ä»…æ”¯æŒ fishaudio ç³»åˆ—æ¨¡å‹
            voice="fishaudio/fish-speech-1.5:benjamin", # ç³»ç»Ÿé¢„ç½®éŸ³è‰²
            # ç”¨æˆ·è¾“å…¥ä¿¡æ¯  "å­™æ‚Ÿç©ºèº«ç©¿é‡‘è‰²æˆ˜ç”²ï¼Œæ‰‹æŒé‡‘ç®æ£’ï¼Œçœ¼ç¥é”åˆ©"
            input=f"{text}",
            response_format="mp3" # æ”¯æŒ mp3, wav, pcm, opus æ ¼å¼
            ) as response:
                response.stream_to_file(self.audio_save_file)
            
            if response.status_code == 200:
                self.audio_text = text
                print(f"æ–‡æœ¬å·²è½¬ä¸ºè¯­éŸ³: {self.audio_save_file}")
                # return f"æ–‡æœ¬è½¬è¯­éŸ³å·²å®Œæˆã€‚"
            else:
                print("æ–‡æœ¬è½¬è¯­éŸ³å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š", response.status_code)
        except Exception as e:
            return f"æ–‡æœ¬è½¬è¯­éŸ³æ—¶å‡ºé”™: {str(e)}"

    def generate_image(self, prompt: str) -> str:
        """
        ä½¿ç”¨APIç”Ÿæˆå›¾åƒ
        """
        if not self.image_url is None:
            print(f"å›¾åƒå·²ç”Ÿæˆ:  {self.image_url}")
            return 
        try:
            # ä½¿ç”¨Prompt Engineerä¼˜åŒ–æç¤ºè¯
            optimized_prompt = self.prompt_engineer.optimize_image_prompt(prompt)
            print(f"ä¼˜åŒ–åçš„å›¾åƒæç¤ºè¯: {optimized_prompt}")

            ## create an image of superman in a tense, action-packed scene, with explosive energy and bold dynamic composition, in the style of Ross Tran
            url = "https://api.siliconflow.cn/v1/images/generations"
            payload = {
                "model": "stabilityai/stable-diffusion-3-5-large",
                "prompt": f"{optimized_prompt}",
                "negative_prompt": "<string>",
                "image_size": "1024x1024",
                "batch_size": 1,
                "seed": 4999999999,
                "num_inference_steps": 20,
                "guidance_scale": 7.5,
                "prompt_enhancement": False
            }

            headers = {
                "Authorization": f"Bearer {self.__api_key}",
                "Content-Type": "application/json"
            }
            response = requests.request("POST", url, json=payload, headers=headers)
            if response.status_code == 200:
                data = response.json()
                self.image_url = data['data'][0]['url']
                print(f"å›¾åƒå·²ç”Ÿæˆ: {self.image_url}")
                # return f"å›¾åƒå·²ç”Ÿæˆã€‚"
                # return f"å›¾åƒå·²ç”Ÿæˆå·²å®Œæˆã€‚ç»§ç»­ä¸‹ä¸€ä¸ªä»»åŠ¡"
            else:
                print("ç”Ÿæˆå›¾åƒå¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š", response.status_code)

        except Exception as e:
            return f"ç”Ÿæˆå›¾åƒæ—¶å‡ºé”™: {str(e)}"
    
    def chat(self, user_input: str) -> dict:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶è¿”å›é€‚å½“çš„å“åº”
        """
        # åˆ›å»ºæç¤ºæ¥å¸®åŠ©agentç†è§£å¦‚ä½•å¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚
        prompt = f"""
        ç”¨æˆ·è¾“å…¥: {user_input}
        
        è¯·æ ¹æ®ä»¥ä¸‹è§„åˆ™å¤„ç†è¿™ä¸ªè¯·æ±‚:
        1. å¦‚æœæ˜¯çŸ¥è¯†ç›¸å…³çš„é—®é¢˜ï¼Œä½¿ç”¨rag_toolæŸ¥è¯¢çŸ¥è¯†åº“
        2. å¦‚æœç”¨æˆ·è¦æ±‚è¯­éŸ³è¾“å‡ºï¼Œä½¿ç”¨tts_toolè½¬æ¢æ–‡æœ¬
        3. å¦‚æœç”¨æˆ·è¦æ±‚ç”Ÿæˆå›¾åƒï¼Œä½¿ç”¨image_toolç”Ÿæˆ

        æ ¹æ®éœ€æ±‚è¯·é€‰æ‹©åˆé€‚çš„å·¥å…·å¹¶æ‰§è¡Œæ“ä½œï¼Œå¯èƒ½éœ€è¦å¤šä¸ªå·¥å…·ã€‚
        """
        self.image_url = None
        self.audio_text = None
        response = self.agent.chat(prompt)
        response_dict = {"response": str(response), "image_url": self.image_url, "audio_text": self.audio_text }
        return response_dict


if __name__ == "__main__":
    ## load wulewule agent 
    wulewule_assistant = load_wulewule_agent()

    ## streamlit setting
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
        
    # åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
    with st.sidebar:
        st.markdown("## æ‚Ÿäº†æ‚Ÿäº†ğŸ’¡")
        logo_path = "assets/sd_wulewule.webp"
        if os.path.exists(logo_path):
            image = Image.open(logo_path)
            st.image(image, caption='wulewule')
        "[InternLM](https://github.com/InternLM)"
        "[æ‚Ÿäº†æ‚Ÿäº†](https://github.com/xzyun2011/wulewule.git)"

        # åˆ›å»ºä¸€ä¸ªæ ‡é¢˜
    st.title("æ‚Ÿäº†æ‚Ÿäº†ï¼šé»‘ç¥è¯æ‚Ÿç©ºAIåŠ©æ‰‹ğŸ’")

    # éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
    for msg in st.session_state.messages:
        st.chat_message("user").write(msg["user"])
        assistant_res = msg["assistant"]
        if isinstance(assistant_res, str):
            st.chat_message("assistant").write(assistant_res)
        elif isinstance(assistant_res, dict):
            image_url = assistant_res["image_url"]
            audio_text = assistant_res["audio_text"]
            st.chat_message("assistant").write(assistant_res["response"])
            if image_url:
                # ä½¿ç”¨st.imageå±•ç¤ºURLå›¾åƒï¼Œå¹¶è®¾ç½®ä½¿ç”¨åˆ—å®½
                st.image( image_url, width=256 )
            if audio_text:
                # ä½¿ç”¨st.audioå‡½æ•°æ’­æ”¾éŸ³é¢‘
                st.audio("audio.mp3")
                st.write(f"è¯­éŸ³å†…å®¹ä¸º: {audio_text}")


    # Get user input #ä½ è§‰å¾—æ‚Ÿç©ºé•¿å•¥æ ·ï¼ŒæŒ‰ä½ çš„æƒ³æ³•ç”»ä¸€ä¸ª
    if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼Œæ¢è¡Œä½¿ç”¨Shfit+Enterã€‚"):
        # Display user input
        st.chat_message("user").write(prompt)
        ## åˆå§‹åŒ–å®Œæ•´çš„å›ç­”å­—ç¬¦ä¸²
        full_answer = ""
        with st.chat_message('robot'):
            message_placeholder = st.empty()
            response_dict = wulewule_assistant.chat(prompt)
            image_url = response_dict["image_url"]
            audio_text = response_dict["audio_text"]
            for cur_response in response_dict["response"]:
                full_answer += cur_response
                # Display robot response in chat message container
                message_placeholder.markdown(full_answer + 'â–Œ')
            message_placeholder.markdown(full_answer)
        # å°†é—®ç­”ç»“æœæ·»åŠ åˆ° session_state çš„æ¶ˆæ¯å†å²ä¸­
        st.session_state.messages.append({"user": prompt, "assistant": response_dict})
        if image_url:
            # ä½¿ç”¨st.imageå±•ç¤ºURLå›¾åƒï¼Œå¹¶è®¾ç½®ä½¿ç”¨åˆ—å®½
            st.image( image_url, width=256 )

        if audio_text:
            # ä½¿ç”¨st.audioå‡½æ•°æ’­æ”¾éŸ³é¢‘
            st.audio("audio.mp3")
            st.write(f"è¯­éŸ³å†…å®¹ä¸º: {audio_text}")
        torch.cuda.empty_cache()
